{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 979/414113 [00:00<00:42, 9788.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:38<00:00, 10672.39it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/miha/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 102502400/102502400 [00:09<00:00, 10466355.83it/s]\n",
      "/home/miha/anaconda3/envs/cvnd/lib/python3.6/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/home/miha/scm/r/hub/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size  = 10          # batch size\n",
    "vocab_threshold = 10        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size  = 300           # dimensionality of image and word embeddings\n",
    "hidden_size = 128          # number of features in hidden state of the RNN decoder\n",
    "\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         cocoapi_loc='/home/miha/scm/r/hub/')\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = Adam(params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 2.5141, Perplexity: 12.3557\n",
      "Epoch [1/3], Step [200/41412], Loss: 1.8782, Perplexity: 6.54159\n",
      "Epoch [1/3], Step [300/41412], Loss: 2.9659, Perplexity: 19.4118\n",
      "Epoch [1/3], Step [400/41412], Loss: 2.2986, Perplexity: 9.96069\n",
      "Epoch [1/3], Step [500/41412], Loss: 2.5581, Perplexity: 12.9109\n",
      "Epoch [1/3], Step [600/41412], Loss: 2.4900, Perplexity: 12.0616\n",
      "Epoch [1/3], Step [700/41412], Loss: 2.3223, Perplexity: 10.1987\n",
      "Epoch [1/3], Step [800/41412], Loss: 2.4158, Perplexity: 11.1992\n",
      "Epoch [1/3], Step [900/41412], Loss: 2.8623, Perplexity: 17.5009\n",
      "Epoch [1/3], Step [1000/41412], Loss: 2.4447, Perplexity: 11.5272\n",
      "Epoch [1/3], Step [1100/41412], Loss: 2.6011, Perplexity: 13.4787\n",
      "Epoch [1/3], Step [1200/41412], Loss: 2.3943, Perplexity: 10.9610\n",
      "Epoch [1/3], Step [1300/41412], Loss: 3.0129, Perplexity: 20.3458\n",
      "Epoch [1/3], Step [1400/41412], Loss: 2.1944, Perplexity: 8.97437\n",
      "Epoch [1/3], Step [1500/41412], Loss: 2.8626, Perplexity: 17.5070\n",
      "Epoch [1/3], Step [1600/41412], Loss: 2.0884, Perplexity: 8.07215\n",
      "Epoch [1/3], Step [1700/41412], Loss: 2.5031, Perplexity: 12.2206\n",
      "Epoch [1/3], Step [1800/41412], Loss: 2.2800, Perplexity: 9.77702\n",
      "Epoch [1/3], Step [1900/41412], Loss: 2.5037, Perplexity: 12.22755\n",
      "Epoch [1/3], Step [2000/41412], Loss: 2.6812, Perplexity: 14.6026\n",
      "Epoch [1/3], Step [2100/41412], Loss: 2.1795, Perplexity: 8.84225\n",
      "Epoch [1/3], Step [2200/41412], Loss: 2.4677, Perplexity: 11.7951\n",
      "Epoch [1/3], Step [2300/41412], Loss: 2.6814, Perplexity: 14.6056\n",
      "Epoch [1/3], Step [2400/41412], Loss: 2.5293, Perplexity: 12.5452\n",
      "Epoch [1/3], Step [2500/41412], Loss: 2.3927, Perplexity: 10.9430\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.3977, Perplexity: 10.9980\n",
      "Epoch [1/3], Step [2700/41412], Loss: 2.5487, Perplexity: 12.7906\n",
      "Epoch [1/3], Step [2800/41412], Loss: 2.3268, Perplexity: 10.2456\n",
      "Epoch [1/3], Step [2900/41412], Loss: 2.8434, Perplexity: 17.1749\n",
      "Epoch [1/3], Step [3000/41412], Loss: 3.1600, Perplexity: 23.5711\n",
      "Epoch [1/3], Step [3100/41412], Loss: 2.3831, Perplexity: 10.8384\n",
      "Epoch [1/3], Step [3200/41412], Loss: 2.1927, Perplexity: 8.95900\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.7407, Perplexity: 15.4973\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.7990, Perplexity: 16.4282\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.3007, Perplexity: 9.981507\n",
      "Epoch [1/3], Step [3600/41412], Loss: 1.7400, Perplexity: 5.69762\n",
      "Epoch [1/3], Step [3700/41412], Loss: 1.9415, Perplexity: 6.96934\n",
      "Epoch [1/3], Step [3800/41412], Loss: 2.2034, Perplexity: 9.05569\n",
      "Epoch [1/3], Step [3900/41412], Loss: 3.1076, Perplexity: 22.3663\n",
      "Epoch [1/3], Step [4000/41412], Loss: 2.7188, Perplexity: 15.1614\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.4000, Perplexity: 11.0236\n",
      "Epoch [1/3], Step [4200/41412], Loss: 2.3239, Perplexity: 10.2152\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.5896, Perplexity: 13.3247\n",
      "Epoch [1/3], Step [4400/41412], Loss: 2.4976, Perplexity: 12.1531\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.0029, Perplexity: 7.41051\n",
      "Epoch [1/3], Step [4600/41412], Loss: 2.7672, Perplexity: 15.9141\n",
      "Epoch [1/3], Step [4700/41412], Loss: 2.5658, Perplexity: 13.0111\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.4622, Perplexity: 11.7300\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.4492, Perplexity: 11.5788\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.8196, Perplexity: 16.7694\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.1310, Perplexity: 8.42344\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.3950, Perplexity: 10.9679\n",
      "Epoch [1/3], Step [5300/41412], Loss: 2.4721, Perplexity: 11.8476\n",
      "Epoch [1/3], Step [5400/41412], Loss: 2.9145, Perplexity: 18.4390\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.7577, Perplexity: 15.7635\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.4437, Perplexity: 11.5154\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.0786, Perplexity: 7.99316\n",
      "Epoch [1/3], Step [5800/41412], Loss: 2.2214, Perplexity: 9.22067\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.3039, Perplexity: 10.0133\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.6748, Perplexity: 14.5089\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.2948, Perplexity: 9.92250\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.7098, Perplexity: 15.0263\n",
      "Epoch [1/3], Step [6300/41412], Loss: 2.1544, Perplexity: 8.62299\n",
      "Epoch [1/3], Step [6400/41412], Loss: 2.2113, Perplexity: 9.12773\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.1802, Perplexity: 8.84849\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.3924, Perplexity: 10.9396\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.8794, Perplexity: 17.8034\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.4850, Perplexity: 12.0006\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.0982, Perplexity: 8.15179\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.6637, Perplexity: 14.3496\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.0016, Perplexity: 7.40123\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.7016, Perplexity: 14.9031\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.2954, Perplexity: 9.92829\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.7350, Perplexity: 15.4097\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.2421, Perplexity: 9.41308\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.6503, Perplexity: 14.1589\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.6539, Perplexity: 14.2097\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.3050, Perplexity: 10.0245\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.6602, Perplexity: 14.2987\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.9844, Perplexity: 19.7745\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.4064, Perplexity: 11.0942\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.2138, Perplexity: 9.15072\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.1831, Perplexity: 8.87366\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.4793, Perplexity: 11.9332\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.4682, Perplexity: 11.8009\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.3513, Perplexity: 10.4990\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.7442, Perplexity: 15.5519\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.3980, Perplexity: 11.0011\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.7148, Perplexity: 15.1020\n",
      "Epoch [1/3], Step [9000/41412], Loss: 3.0254, Perplexity: 20.6025\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.3864, Perplexity: 10.8746\n",
      "Epoch [1/3], Step [9200/41412], Loss: 1.7451, Perplexity: 5.72653\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.4776, Perplexity: 11.9129\n",
      "Epoch [1/3], Step [9400/41412], Loss: 2.3202, Perplexity: 10.1776\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.3172, Perplexity: 10.1468\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.3702, Perplexity: 10.6996\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.4644, Perplexity: 11.7561\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.2225, Perplexity: 9.23023\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.6716, Perplexity: 14.4630\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.6519, Perplexity: 14.1811\n",
      "Epoch [1/3], Step [10100/41412], Loss: 3.0295, Perplexity: 20.6865\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.1092, Perplexity: 8.24164\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.2237, Perplexity: 9.24174\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.1325, Perplexity: 8.43614\n",
      "Epoch [1/3], Step [10500/41412], Loss: 1.9635, Perplexity: 7.12401\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.1135, Perplexity: 8.27722\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.6919, Perplexity: 14.7602\n",
      "Epoch [1/3], Step [10800/41412], Loss: 1.9820, Perplexity: 7.25752\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.7779, Perplexity: 16.0857\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.3310, Perplexity: 10.2880\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.6452, Perplexity: 14.0862\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.0347, Perplexity: 7.65028\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.5364, Perplexity: 12.6347\n",
      "Epoch [1/3], Step [11400/41412], Loss: 2.4964, Perplexity: 12.1386\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.4808, Perplexity: 11.9503\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.2275, Perplexity: 9.27683\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.3190, Perplexity: 10.1654\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.2791, Perplexity: 9.76798\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.4276, Perplexity: 11.3321\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.0140, Perplexity: 7.49334\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.6620, Perplexity: 14.3246\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.3056, Perplexity: 10.0297\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.7311, Perplexity: 15.3496\n",
      "Epoch [1/3], Step [12400/41412], Loss: 2.8171, Perplexity: 16.7283\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.5318, Perplexity: 12.5765\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.0178, Perplexity: 7.52188\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.3244, Perplexity: 10.2202\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.3956, Perplexity: 10.9748\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.2498, Perplexity: 9.48631\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.1274, Perplexity: 8.39294\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.7144, Perplexity: 15.0961\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.0229, Perplexity: 7.55995\n",
      "Epoch [1/3], Step [13300/41412], Loss: 1.9783, Perplexity: 7.23038\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.2153, Perplexity: 9.16436\n",
      "Epoch [1/3], Step [13500/41412], Loss: 1.9800, Perplexity: 7.24279\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.4354, Perplexity: 11.4208\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.3019, Perplexity: 9.99369\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.3722, Perplexity: 10.7206\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.2574, Perplexity: 9.55771\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.0141, Perplexity: 7.49405\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.5236, Perplexity: 12.4732\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.2047, Perplexity: 9.06713\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.1896, Perplexity: 8.93185\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.1916, Perplexity: 8.94983\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.3688, Perplexity: 10.6847\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.5953, Perplexity: 13.4005\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.3703, Perplexity: 10.7010\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.2366, Perplexity: 9.36152\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.5796, Perplexity: 13.1919\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.3753, Perplexity: 10.7548\n",
      "Epoch [1/3], Step [15100/41412], Loss: 3.4151, Perplexity: 30.4209\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.9398, Perplexity: 18.9128\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.4620, Perplexity: 11.7287\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.3021, Perplexity: 9.99506\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.0502, Perplexity: 7.76932\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.2659, Perplexity: 9.64000\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.6977, Perplexity: 14.8453\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.6664, Perplexity: 14.3885\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.3707, Perplexity: 10.7051\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.2342, Perplexity: 9.33912\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.2143, Perplexity: 9.15475\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.3484, Perplexity: 10.4684\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.8439, Perplexity: 17.18281\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.1249, Perplexity: 8.37219\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.6314, Perplexity: 13.8934\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.2155, Perplexity: 9.16586\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.2623, Perplexity: 9.60510\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.5052, Perplexity: 12.2459\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.0695, Perplexity: 7.92068\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.7323, Perplexity: 15.3679\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.5686, Perplexity: 13.0475\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.5829, Perplexity: 13.2352\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.0707, Perplexity: 7.93039\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.5497, Perplexity: 12.8029\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.1119, Perplexity: 8.26401\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.9863, Perplexity: 19.8116\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.9053, Perplexity: 18.2701\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.3189, Perplexity: 10.1647\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.5133, Perplexity: 12.3461\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.4076, Perplexity: 11.1074\n",
      "Epoch [1/3], Step [18100/41412], Loss: 3.0038, Perplexity: 20.1615\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.2618, Perplexity: 9.60044\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.1972, Perplexity: 8.99952\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.4801, Perplexity: 11.9421\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.0811, Perplexity: 8.01360\n",
      "Epoch [1/3], Step [18600/41412], Loss: 3.0783, Perplexity: 21.7210\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.4688, Perplexity: 11.8081\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.5193, Perplexity: 12.4193\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.1774, Perplexity: 8.82370\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.3946, Perplexity: 10.9642\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.6114, Perplexity: 13.6182\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.3387, Perplexity: 10.3681\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.2695, Perplexity: 9.67442\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.2434, Perplexity: 9.42522\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.2666, Perplexity: 9.64660\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.2680, Perplexity: 9.66019\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.5739, Perplexity: 13.1173\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.6308, Perplexity: 13.8856\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.4773, Perplexity: 11.9092\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.2652, Perplexity: 9.63340\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.4174, Perplexity: 11.2169\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.2526, Perplexity: 9.51291\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.2569, Perplexity: 9.55397\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.3841, Perplexity: 10.84978\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.0992, Perplexity: 8.15973\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.1057, Perplexity: 8.21266\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.5266, Perplexity: 12.5112\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.4995, Perplexity: 12.1758\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.6820, Perplexity: 14.6140\n",
      "Epoch [1/3], Step [21000/41412], Loss: 3.3905, Perplexity: 29.6796\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.5542, Perplexity: 12.8606\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.1767, Perplexity: 8.81767\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.4382, Perplexity: 11.4529\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.1556, Perplexity: 8.63296\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.0907, Perplexity: 8.09078\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.5825, Perplexity: 13.2307\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.0393, Perplexity: 7.68495\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.1141, Perplexity: 8.28203\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.3947, Perplexity: 10.9648\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.1742, Perplexity: 8.79512\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.5148, Perplexity: 12.3639\n",
      "Epoch [1/3], Step [22200/41412], Loss: 1.6042, Perplexity: 4.97414\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.5182, Perplexity: 12.4063\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.6618, Perplexity: 14.3224\n",
      "Epoch [1/3], Step [22500/41412], Loss: 3.1450, Perplexity: 23.2187\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.1760, Perplexity: 8.81120\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.4610, Perplexity: 11.7163\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.3885, Perplexity: 10.8968\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.8639, Perplexity: 17.5299\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.7838, Perplexity: 16.1807\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.6443, Perplexity: 14.0729\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.5981, Perplexity: 13.4376\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.3518, Perplexity: 10.5049\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.1611, Perplexity: 8.68094\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.1966, Perplexity: 8.99426\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.7035, Perplexity: 14.9320\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.0424, Perplexity: 7.70910\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.5076, Perplexity: 12.2758\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.3554, Perplexity: 10.5419\n",
      "Epoch [1/3], Step [24000/41412], Loss: 3.0360, Perplexity: 20.8227\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.1138, Perplexity: 8.28015\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.9470, Perplexity: 19.0480\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.4064, Perplexity: 11.0935\n",
      "Epoch [1/3], Step [24400/41412], Loss: 1.8810, Perplexity: 6.56014\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.2637, Perplexity: 9.61843\n",
      "Epoch [1/3], Step [24600/41412], Loss: 2.3574, Perplexity: 10.5637\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.1851, Perplexity: 8.89184\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.7811, Perplexity: 16.1369\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.5487, Perplexity: 12.7910\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.6825, Perplexity: 14.6209\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.2883, Perplexity: 9.85786\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.5722, Perplexity: 13.0945\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.4980, Perplexity: 12.1582\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.2626, Perplexity: 9.60789\n",
      "Epoch [1/3], Step [25500/41412], Loss: 1.8435, Perplexity: 6.31888\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.6802, Perplexity: 14.58779\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.8849, Perplexity: 17.9017\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.0626, Perplexity: 7.86611\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.3106, Perplexity: 10.0804\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.7455, Perplexity: 15.5716\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.5305, Perplexity: 12.5598\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.5262, Perplexity: 12.5059\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.4492, Perplexity: 11.5789\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.4483, Perplexity: 11.5691\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.1515, Perplexity: 8.59802\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.4067, Perplexity: 11.0969\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.2347, Perplexity: 9.34358\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.6285, Perplexity: 13.8526\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.6127, Perplexity: 13.6358\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.7058, Perplexity: 14.9664\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.2092, Perplexity: 9.10810\n",
      "Epoch [1/3], Step [27200/41412], Loss: 3.0981, Perplexity: 22.1555\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.8080, Perplexity: 16.5772\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.3337, Perplexity: 10.3157\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.4237, Perplexity: 11.2870\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.5873, Perplexity: 13.2934\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.1442, Perplexity: 8.53482\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.5381, Perplexity: 12.6557\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.3438, Perplexity: 10.4212\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.1001, Perplexity: 8.16678\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.6311, Perplexity: 13.8894\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.2156, Perplexity: 9.16733\n",
      "Epoch [1/3], Step [28300/41412], Loss: 3.1157, Perplexity: 22.5489\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.7807, Perplexity: 16.1307\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.0106, Perplexity: 7.46781\n",
      "Epoch [1/3], Step [28600/41412], Loss: 1.9969, Perplexity: 7.36619\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.4781, Perplexity: 11.9189\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.3374, Perplexity: 10.3545\n",
      "Epoch [1/3], Step [28900/41412], Loss: 3.4777, Perplexity: 32.3853\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.6240, Perplexity: 13.7908\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.1061, Perplexity: 8.21611\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.6339, Perplexity: 13.9273\n",
      "Epoch [1/3], Step [29300/41412], Loss: 1.7672, Perplexity: 5.85463\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.4069, Perplexity: 11.0999\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.5284, Perplexity: 12.5331\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.5156, Perplexity: 12.3737\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.1278, Perplexity: 8.39649\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.1870, Perplexity: 8.90841\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.1407, Perplexity: 8.50519\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.7658, Perplexity: 15.8911\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.2579, Perplexity: 9.56286\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.7641, Perplexity: 15.8642\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.6419, Perplexity: 14.0403\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.6523, Perplexity: 14.1872\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.5426, Perplexity: 12.7132\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.6102, Perplexity: 13.6016\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.4532, Perplexity: 11.6256\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.4065, Perplexity: 11.0955\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.6337, Perplexity: 13.9248\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.4758, Perplexity: 11.8916\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.9125, Perplexity: 18.4027\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.0066, Perplexity: 7.43797\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.6992, Perplexity: 14.8684\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.5095, Perplexity: 12.2991\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.4881, Perplexity: 12.0380\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.7594, Perplexity: 15.7897\n",
      "Epoch [1/3], Step [31700/41412], Loss: 3.3241, Perplexity: 27.7726\n",
      "Epoch [1/3], Step [31800/41412], Loss: 1.9271, Perplexity: 6.86985\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.3855, Perplexity: 10.8647\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.3631, Perplexity: 10.6243\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.1254, Perplexity: 8.37605\n",
      "Epoch [1/3], Step [32200/41412], Loss: 2.4713, Perplexity: 11.8380\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.6731, Perplexity: 14.4851\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.3331, Perplexity: 10.3095\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.3027, Perplexity: 10.0010\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.2538, Perplexity: 9.52417\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.7416, Perplexity: 15.5117\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.2617, Perplexity: 9.59909\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.5632, Perplexity: 12.9770\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.4654, Perplexity: 11.7685\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.1797, Perplexity: 8.84370\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.5305, Perplexity: 12.5602\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.3428, Perplexity: 10.4108\n",
      "Epoch [1/3], Step [33400/41412], Loss: 1.9171, Perplexity: 6.80155\n",
      "Epoch [1/3], Step [33500/41412], Loss: 3.2360, Perplexity: 25.4314\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.9994, Perplexity: 20.0742\n",
      "Epoch [1/3], Step [33700/41412], Loss: 1.9574, Perplexity: 7.08119\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.5211, Perplexity: 12.4426\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.9248, Perplexity: 18.6312\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.2328, Perplexity: 9.32568\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.2024, Perplexity: 9.04676\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.4815, Perplexity: 11.9594\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.5818, Perplexity: 13.2210\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.9658, Perplexity: 19.4110\n",
      "Epoch [1/3], Step [34500/41412], Loss: 1.8349, Perplexity: 6.26484\n",
      "Epoch [1/3], Step [34600/41412], Loss: 2.4085, Perplexity: 11.1168\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.5101, Perplexity: 12.3064\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.6211, Perplexity: 13.7512\n",
      "Epoch [1/3], Step [34900/41412], Loss: 3.4238, Perplexity: 30.6853\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.4349, Perplexity: 11.4146\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.3311, Perplexity: 10.2892\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.5094, Perplexity: 12.2981\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.2184, Perplexity: 9.19302\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.1584, Perplexity: 8.65699\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.2082, Perplexity: 9.09940\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.3182, Perplexity: 10.1574\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.3484, Perplexity: 10.4693\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.0735, Perplexity: 7.95261\n",
      "Epoch [1/3], Step [35900/41412], Loss: 3.0653, Perplexity: 21.4399\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.6739, Perplexity: 14.4959\n",
      "Epoch [1/3], Step [36100/41412], Loss: 3.4273, Perplexity: 30.7934\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.4001, Perplexity: 11.0246\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.5024, Perplexity: 12.2124\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.1339, Perplexity: 8.44809\n",
      "Epoch [1/3], Step [36500/41412], Loss: 1.9508, Perplexity: 7.03462\n",
      "Epoch [1/3], Step [36600/41412], Loss: 1.9399, Perplexity: 6.95804\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.8939, Perplexity: 18.0630\n",
      "Epoch [1/3], Step [36800/41412], Loss: 1.9746, Perplexity: 7.20403\n",
      "Epoch [1/3], Step [36900/41412], Loss: 3.0077, Perplexity: 20.2409\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.2721, Perplexity: 9.69985\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.1583, Perplexity: 8.65670\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.0305, Perplexity: 7.61803\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.9689, Perplexity: 19.4695\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.1228, Perplexity: 8.35457\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.1792, Perplexity: 8.83943\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.6863, Perplexity: 14.6770\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.2638, Perplexity: 9.62008\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.4115, Perplexity: 11.1505\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.4177, Perplexity: 11.2200\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.1411, Perplexity: 8.50919\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.4544, Perplexity: 11.6392\n",
      "Epoch [1/3], Step [38200/41412], Loss: 3.1439, Perplexity: 23.1933\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.6905, Perplexity: 14.7391\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.3160, Perplexity: 10.1347\n",
      "Epoch [1/3], Step [38500/41412], Loss: 1.9644, Perplexity: 7.13050\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.4647, Perplexity: 11.7604\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.3553, Perplexity: 10.5415\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.4133, Perplexity: 11.1704\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.0217, Perplexity: 7.55130\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.4820, Perplexity: 11.9652\n",
      "Epoch [1/3], Step [39100/41412], Loss: 2.2865, Perplexity: 9.84026\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.5002, Perplexity: 12.1844\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.5548, Perplexity: 12.8690\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.0551, Perplexity: 7.80794\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.3595, Perplexity: 10.5855\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.3507, Perplexity: 10.4933\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.2713, Perplexity: 9.69184\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.3480, Perplexity: 10.4647\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.2114, Perplexity: 9.12895\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.2866, Perplexity: 9.84151\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.7842, Perplexity: 16.1871\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.1082, Perplexity: 8.23341\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.1909, Perplexity: 8.94306\n",
      "Epoch [1/3], Step [40400/41412], Loss: 1.8763, Perplexity: 6.52967\n",
      "Epoch [1/3], Step [40500/41412], Loss: 1.9684, Perplexity: 7.15941\n",
      "Epoch [1/3], Step [40600/41412], Loss: 1.9657, Perplexity: 7.13998\n",
      "Epoch [1/3], Step [40700/41412], Loss: 1.8748, Perplexity: 6.51989\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.0719, Perplexity: 7.93992\n",
      "Epoch [1/3], Step [40900/41412], Loss: 3.6371, Perplexity: 37.9833\n",
      "Epoch [1/3], Step [41000/41412], Loss: 3.1468, Perplexity: 23.26144\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.6185, Perplexity: 13.7146\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.2758, Perplexity: 9.73596\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.3823, Perplexity: 10.8296\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.6607, Perplexity: 14.3063\n",
      "Epoch [2/3], Step [100/41412], Loss: 1.9637, Perplexity: 7.1253049\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.1426, Perplexity: 8.52191\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.1149, Perplexity: 8.28866\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.4426, Perplexity: 11.5025\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.4282, Perplexity: 11.3387\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.0722, Perplexity: 7.94252\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.5591, Perplexity: 12.9242\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.8346, Perplexity: 17.0241\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.0213, Perplexity: 7.54818\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.7279, Perplexity: 15.3014\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.0335, Perplexity: 7.64116\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.5413, Perplexity: 12.6963\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.3648, Perplexity: 10.6418\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.3665, Perplexity: 10.6601\n",
      "Epoch [2/3], Step [1500/41412], Loss: 1.9594, Perplexity: 7.09506\n",
      "Epoch [2/3], Step [1600/41412], Loss: 1.9810, Perplexity: 7.25012\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.0633, Perplexity: 7.87227\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.4296, Perplexity: 11.3546\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.1885, Perplexity: 8.92176\n",
      "Epoch [2/3], Step [2000/41412], Loss: 1.9719, Perplexity: 7.18444\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.2360, Perplexity: 9.35627\n",
      "Epoch [2/3], Step [2200/41412], Loss: 1.9038, Perplexity: 6.71139\n",
      "Epoch [2/3], Step [2300/41412], Loss: 2.3466, Perplexity: 10.44957\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.3314, Perplexity: 10.2928\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.5916, Perplexity: 13.3507\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.4905, Perplexity: 12.0669\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.2782, Perplexity: 9.75943\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.1330, Perplexity: 8.44023\n",
      "Epoch [2/3], Step [2900/41412], Loss: 1.9268, Perplexity: 6.86724\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.3651, Perplexity: 10.6454\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.5557, Perplexity: 12.8807\n",
      "Epoch [2/3], Step [3200/41412], Loss: 1.8840, Perplexity: 6.57954\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.3234, Perplexity: 10.2105\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.2517, Perplexity: 9.50417\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.1834, Perplexity: 8.87632\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.9127, Perplexity: 18.4061\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.0099, Perplexity: 7.46267\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.3957, Perplexity: 10.9764\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.7463, Perplexity: 15.5846\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.6264, Perplexity: 13.8236\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.3913, Perplexity: 10.9282\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.5189, Perplexity: 12.4155\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.8444, Perplexity: 17.1911\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.4126, Perplexity: 11.1628\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.7156, Perplexity: 15.1141\n",
      "Epoch [2/3], Step [4600/41412], Loss: 1.9749, Perplexity: 7.20626\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.7248, Perplexity: 15.2537\n",
      "Epoch [2/3], Step [4800/41412], Loss: 1.9789, Perplexity: 7.23501\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.5593, Perplexity: 12.9272\n",
      "Epoch [2/3], Step [5000/41412], Loss: 3.0086, Perplexity: 20.2581\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.1907, Perplexity: 8.94125\n",
      "Epoch [2/3], Step [5200/41412], Loss: 3.3425, Perplexity: 28.2889\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.0799, Perplexity: 8.00399\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.3565, Perplexity: 10.5542\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.3888, Perplexity: 10.9004\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.6574, Perplexity: 14.2594\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.2545, Perplexity: 9.53096\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.1398, Perplexity: 8.49731\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.2381, Perplexity: 9.37599\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.3620, Perplexity: 10.6119\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.9404, Perplexity: 18.9237\n",
      "Epoch [2/3], Step [6200/41412], Loss: 1.9809, Perplexity: 7.24944\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.5676, Perplexity: 13.0339\n",
      "Epoch [2/3], Step [6400/41412], Loss: 3.6404, Perplexity: 38.1079\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.2838, Perplexity: 9.81390\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.4681, Perplexity: 11.8005\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.5160, Perplexity: 12.3791\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6579, Perplexity: 14.2660\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.3993, Perplexity: 11.0156\n",
      "Epoch [2/3], Step [7000/41412], Loss: 1.9212, Perplexity: 6.82929\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.3282, Perplexity: 10.2593\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.0439, Perplexity: 7.72099\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.3552, Perplexity: 10.5407\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.1818, Perplexity: 8.86190\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.3294, Perplexity: 10.2714\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.6866, Perplexity: 14.6812\n",
      "Epoch [2/3], Step [7700/41412], Loss: 3.1074, Perplexity: 22.3637\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.1530, Perplexity: 8.61034\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.2066, Perplexity: 9.08506\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.5178, Perplexity: 12.4011\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.6066, Perplexity: 13.5533\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.4242, Perplexity: 11.2934\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.3071, Perplexity: 10.0453\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.7654, Perplexity: 15.8860\n",
      "Epoch [2/3], Step [8500/41412], Loss: 1.8749, Perplexity: 6.52032\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.4451, Perplexity: 11.5317\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.4054, Perplexity: 11.0832\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.1435, Perplexity: 8.52897\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.3504, Perplexity: 10.4894\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.1210, Perplexity: 8.33969\n",
      "Epoch [2/3], Step [9100/41412], Loss: 1.9071, Perplexity: 6.73366\n",
      "Epoch [2/3], Step [9200/41412], Loss: 2.4671, Perplexity: 11.7879\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.5980, Perplexity: 13.4363\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.2554, Perplexity: 9.53919\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.6331, Perplexity: 13.9168\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.5345, Perplexity: 12.6096\n",
      "Epoch [2/3], Step [9700/41412], Loss: 1.8543, Perplexity: 6.38746\n",
      "Epoch [2/3], Step [9800/41412], Loss: 1.9098, Perplexity: 6.75197\n",
      "Epoch [2/3], Step [9900/41412], Loss: 1.9874, Perplexity: 7.29658\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.5584, Perplexity: 12.9150\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.6044, Perplexity: 13.5229\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.1945, Perplexity: 8.97563\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.2210, Perplexity: 9.21695\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.9835, Perplexity: 19.7561\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.4833, Perplexity: 11.9807\n",
      "Epoch [2/3], Step [10600/41412], Loss: 2.0633, Perplexity: 7.87222\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.5735, Perplexity: 13.1110\n",
      "Epoch [2/3], Step [10800/41412], Loss: 1.8921, Perplexity: 6.63353\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.6178, Perplexity: 13.7053\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.7137, Perplexity: 15.0854\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.0247, Perplexity: 7.57422\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.6206, Perplexity: 13.7435\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.1414, Perplexity: 8.51101\n",
      "Epoch [2/3], Step [11400/41412], Loss: 2.4293, Perplexity: 11.3507\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.1292, Perplexity: 8.40787\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.9671, Perplexity: 19.4358\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.1124, Perplexity: 8.26804\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.6551, Perplexity: 14.2268\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.3396, Perplexity: 10.3767\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.5210, Perplexity: 12.4408\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.4955, Perplexity: 12.1273\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.1329, Perplexity: 8.43945\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.2285, Perplexity: 9.28620\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.4574, Perplexity: 11.6742\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.1953, Perplexity: 8.98303\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.2681, Perplexity: 9.66117\n",
      "Epoch [2/3], Step [12700/41412], Loss: 1.9381, Perplexity: 6.94550\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.9817, Perplexity: 19.72109\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.2838, Perplexity: 9.81429\n",
      "Epoch [2/3], Step [13000/41412], Loss: 3.0727, Perplexity: 21.5993\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.2521, Perplexity: 9.50771\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.7985, Perplexity: 16.4196\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.0479, Perplexity: 7.75166\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.2206, Perplexity: 9.21303\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.5030, Perplexity: 12.2194\n",
      "Epoch [2/3], Step [13600/41412], Loss: 1.7968, Perplexity: 6.03017\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.3487, Perplexity: 10.47246\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.0211, Perplexity: 7.54659\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.4389, Perplexity: 11.4599\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.1213, Perplexity: 8.34160\n",
      "Epoch [2/3], Step [14100/41412], Loss: 2.1629, Perplexity: 8.69615\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.6571, Perplexity: 14.2552\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.6285, Perplexity: 13.8527\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.5778, Perplexity: 13.1686\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.4743, Perplexity: 11.8729\n",
      "Epoch [2/3], Step [14600/41412], Loss: 1.9670, Perplexity: 7.14945\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.5101, Perplexity: 12.3058\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.8392, Perplexity: 17.1028\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.0938, Perplexity: 8.11555\n",
      "Epoch [2/3], Step [15000/41412], Loss: 1.8903, Perplexity: 6.62133\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.4755, Perplexity: 11.8876\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.3021, Perplexity: 9.99526\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.9229, Perplexity: 18.5959\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.2668, Perplexity: 9.64848\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.4253, Perplexity: 11.3051\n",
      "Epoch [2/3], Step [15600/41412], Loss: 1.9668, Perplexity: 7.14756\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.5548, Perplexity: 12.8684\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.5060, Perplexity: 12.2562\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.8572, Perplexity: 17.4123\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.4428, Perplexity: 11.5053\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.4179, Perplexity: 11.2224\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.4747, Perplexity: 11.8778\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.3959, Perplexity: 10.9780\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.1210, Perplexity: 8.33921\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.2662, Perplexity: 9.64286\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.9863, Perplexity: 19.8120\n",
      "Epoch [2/3], Step [16700/41412], Loss: 2.3256, Perplexity: 10.2333\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.2500, Perplexity: 9.48779\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.4887, Perplexity: 12.0455\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.2857, Perplexity: 9.83272\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.8702, Perplexity: 17.6413\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.3283, Perplexity: 10.2604\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.1056, Perplexity: 8.21198\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.4963, Perplexity: 12.1379\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.3596, Perplexity: 10.58650\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.1798, Perplexity: 8.84435\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.3061, Perplexity: 10.0357\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.1167, Perplexity: 8.30402\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.3773, Perplexity: 10.7757\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.4283, Perplexity: 11.3399\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.5156, Perplexity: 12.3736\n",
      "Epoch [2/3], Step [18200/41412], Loss: 1.8957, Perplexity: 6.65745\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.8404, Perplexity: 17.1219\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.7325, Perplexity: 15.3709\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.0609, Perplexity: 7.85287\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.1289, Perplexity: 8.40533\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.2501, Perplexity: 9.48880\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.1504, Perplexity: 8.58808\n",
      "Epoch [2/3], Step [18900/41412], Loss: 2.5269, Perplexity: 12.5144\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.0190, Perplexity: 7.53078\n",
      "Epoch [2/3], Step [19100/41412], Loss: 2.3057, Perplexity: 10.0309\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.6738, Perplexity: 14.4954\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.4869, Perplexity: 12.0237\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.1220, Perplexity: 8.34803\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.0038, Perplexity: 7.41744\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.3753, Perplexity: 10.7547\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.8487, Perplexity: 17.2651\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.8261, Perplexity: 16.8799\n",
      "Epoch [2/3], Step [19900/41412], Loss: 2.2095, Perplexity: 9.11084\n",
      "Epoch [2/3], Step [20000/41412], Loss: 2.1253, Perplexity: 8.37506\n",
      "Epoch [2/3], Step [20100/41412], Loss: 2.0584, Perplexity: 7.83318\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.2299, Perplexity: 9.29922\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.3138, Perplexity: 10.1124\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.2996, Perplexity: 9.97022\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.6461, Perplexity: 14.0985\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.9605, Perplexity: 19.3085\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.7037, Perplexity: 14.9354\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.0812, Perplexity: 8.014047\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.3459, Perplexity: 10.44269\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.7158, Perplexity: 15.1171\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.4067, Perplexity: 11.0973\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.3408, Perplexity: 10.3892\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.2506, Perplexity: 9.49349\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.0396, Perplexity: 7.68771\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.4823, Perplexity: 11.9689\n",
      "Epoch [2/3], Step [21600/41412], Loss: 2.6308, Perplexity: 13.8855\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.6623, Perplexity: 14.3297\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.2005, Perplexity: 9.029554\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.0707, Perplexity: 7.93065\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.0921, Perplexity: 8.10176\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.6951, Perplexity: 14.8067\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.2250, Perplexity: 9.25327\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.3178, Perplexity: 10.1533\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.4136, Perplexity: 11.1740\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.2627, Perplexity: 9.60874\n",
      "Epoch [2/3], Step [22600/41412], Loss: 3.3845, Perplexity: 29.5021\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.8195, Perplexity: 16.7676\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.1716, Perplexity: 8.77223\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.4958, Perplexity: 12.1320\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.3362, Perplexity: 10.3417\n",
      "Epoch [2/3], Step [23100/41412], Loss: 4.5493, Perplexity: 94.5693\n",
      "Epoch [2/3], Step [23200/41412], Loss: 1.9311, Perplexity: 6.89719\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.3651, Perplexity: 10.6449\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.3506, Perplexity: 10.4918\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.8522, Perplexity: 17.3253\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.2845, Perplexity: 9.82129\n",
      "Epoch [2/3], Step [23700/41412], Loss: 1.9109, Perplexity: 6.75892\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.7156, Perplexity: 15.1137\n",
      "Epoch [2/3], Step [23900/41412], Loss: 1.9887, Perplexity: 7.30591\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.4202, Perplexity: 11.2481\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.0609, Perplexity: 7.85322\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.2931, Perplexity: 9.90609\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.6802, Perplexity: 14.5876\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.9461, Perplexity: 19.0319\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.3360, Perplexity: 10.3401\n",
      "Epoch [2/3], Step [24600/41412], Loss: 1.9859, Perplexity: 7.28584\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.3673, Perplexity: 10.6682\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.0625, Perplexity: 7.86567\n",
      "Epoch [2/3], Step [24900/41412], Loss: 1.6365, Perplexity: 5.13691\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.5206, Perplexity: 12.4367\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.6754, Perplexity: 14.5183\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.6806, Perplexity: 14.5944\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.0773, Perplexity: 7.98307\n",
      "Epoch [2/3], Step [25400/41412], Loss: 3.3837, Perplexity: 29.4809\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.7445, Perplexity: 15.5568\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.8257, Perplexity: 16.8733\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.4163, Perplexity: 11.2047\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.6136, Perplexity: 13.6475\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.7098, Perplexity: 15.0266\n",
      "Epoch [2/3], Step [26000/41412], Loss: 3.0609, Perplexity: 21.34684\n",
      "Epoch [2/3], Step [26100/41412], Loss: 1.7943, Perplexity: 6.01536\n",
      "Epoch [2/3], Step [26200/41412], Loss: 1.8788, Perplexity: 6.54541\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.5850, Perplexity: 13.2631\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.1989, Perplexity: 9.01527\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.2262, Perplexity: 9.26471\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.3075, Perplexity: 10.0497\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.3462, Perplexity: 10.4453\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.4360, Perplexity: 11.4272\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.3938, Perplexity: 10.9545\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.7535, Perplexity: 15.6977\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.6652, Perplexity: 14.3703\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.3833, Perplexity: 10.8409\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.4363, Perplexity: 11.4304\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.2053, Perplexity: 9.07342\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.7662, Perplexity: 15.8976\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.1543, Perplexity: 8.62229\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.3634, Perplexity: 10.6268\n",
      "Epoch [2/3], Step [27800/41412], Loss: 3.5808, Perplexity: 35.90163\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.5027, Perplexity: 12.2154\n",
      "Epoch [2/3], Step [28000/41412], Loss: 3.2035, Perplexity: 24.6176\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.3498, Perplexity: 10.48301\n",
      "Epoch [2/3], Step [28200/41412], Loss: 1.7266, Perplexity: 5.62145\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.1761, Perplexity: 8.81164\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.3484, Perplexity: 10.4691\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.2005, Perplexity: 9.02936\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.2235, Perplexity: 9.23926\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.4531, Perplexity: 11.6247\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.4759, Perplexity: 11.8927\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.3994, Perplexity: 11.0167\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.2334, Perplexity: 9.33168\n",
      "Epoch [2/3], Step [29100/41412], Loss: 3.0651, Perplexity: 21.43777\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.2719, Perplexity: 9.69764\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.7588, Perplexity: 15.7814\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.1595, Perplexity: 8.66664\n",
      "Epoch [2/3], Step [29500/41412], Loss: 2.5790, Perplexity: 13.1838\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.4390, Perplexity: 11.4612\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.0569, Perplexity: 7.82142\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.2175, Perplexity: 9.18400\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.4865, Perplexity: 12.0197\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.6446, Perplexity: 14.0777\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.0391, Perplexity: 7.68403\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.2039, Perplexity: 9.06043\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.6359, Perplexity: 13.9558\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.1786, Perplexity: 8.83431\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.3498, Perplexity: 10.4837\n",
      "Epoch [2/3], Step [30600/41412], Loss: 1.8494, Perplexity: 6.35634\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.0430, Perplexity: 7.71348\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.3277, Perplexity: 10.2544\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.2116, Perplexity: 9.13062\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.8327, Perplexity: 16.9909\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.5568, Perplexity: 12.8939\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.3066, Perplexity: 10.0406\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.1968, Perplexity: 8.99651\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.5374, Perplexity: 12.6473\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.5515, Perplexity: 12.8259\n",
      "Epoch [2/3], Step [31600/41412], Loss: 1.7097, Perplexity: 5.52703\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.4460, Perplexity: 11.5425\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.4728, Perplexity: 11.8560\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.2180, Perplexity: 9.18909\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.3940, Perplexity: 10.9570\n",
      "Epoch [2/3], Step [32100/41412], Loss: 2.1970, Perplexity: 8.99823\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.2536, Perplexity: 9.52155\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.2946, Perplexity: 9.92029\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.3471, Perplexity: 10.4551\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.7380, Perplexity: 15.4567\n",
      "Epoch [2/3], Step [32600/41412], Loss: 3.3516, Perplexity: 28.5482\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.3245, Perplexity: 10.2219\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.2251, Perplexity: 9.25463\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.5683, Perplexity: 13.0442\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.3328, Perplexity: 10.3063\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.4715, Perplexity: 11.8396\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.6077, Perplexity: 13.56716\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.4636, Perplexity: 11.7468\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.1128, Perplexity: 8.27115\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.5117, Perplexity: 12.3260\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.2622, Perplexity: 9.60431\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.5666, Perplexity: 13.0211\n",
      "Epoch [2/3], Step [33800/41412], Loss: 1.9663, Perplexity: 7.144386\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.1663, Perplexity: 8.72638\n",
      "Epoch [2/3], Step [34000/41412], Loss: 1.9737, Perplexity: 7.19764\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.2987, Perplexity: 9.96084\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.7012, Perplexity: 14.8979\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.4613, Perplexity: 11.7205\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.3621, Perplexity: 10.6135\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.7844, Perplexity: 16.1902\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.3388, Perplexity: 10.36921\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.3947, Perplexity: 10.9651\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.4069, Perplexity: 11.0997\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.4435, Perplexity: 11.5131\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.5800, Perplexity: 13.1970\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.7756, Perplexity: 16.0485\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.5447, Perplexity: 12.7390\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.6386, Perplexity: 13.9933\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.5285, Perplexity: 12.5353\n",
      "Epoch [2/3], Step [35500/41412], Loss: 2.5152, Perplexity: 12.3692\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.3205, Perplexity: 10.1805\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.7214, Perplexity: 15.2012\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.2483, Perplexity: 9.47172\n",
      "Epoch [2/3], Step [35900/41412], Loss: 3.3340, Perplexity: 28.0515\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.2143, Perplexity: 9.15462\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.0392, Perplexity: 7.68420\n",
      "Epoch [2/3], Step [36200/41412], Loss: 1.9882, Perplexity: 7.30224\n",
      "Epoch [2/3], Step [36300/41412], Loss: 2.5225, Perplexity: 12.4601\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.7717, Perplexity: 15.9855\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.6286, Perplexity: 13.8542\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.2695, Perplexity: 9.67427\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.2957, Perplexity: 9.93179\n",
      "Epoch [2/3], Step [36800/41412], Loss: 1.8893, Perplexity: 6.61489\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.4451, Perplexity: 11.5322\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.2876, Perplexity: 9.85177\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.3677, Perplexity: 10.6724\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.5815, Perplexity: 13.2172\n",
      "Epoch [2/3], Step [37300/41412], Loss: 3.0241, Perplexity: 20.5758\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.2787, Perplexity: 9.76436\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.0275, Perplexity: 7.59478\n",
      "Epoch [2/3], Step [37600/41412], Loss: 1.7801, Perplexity: 5.93044\n",
      "Epoch [2/3], Step [37700/41412], Loss: 1.9555, Perplexity: 7.06714\n",
      "Epoch [2/3], Step [37800/41412], Loss: 3.1834, Perplexity: 24.1298\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.3595, Perplexity: 10.5855\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.5110, Perplexity: 12.3174\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.5287, Perplexity: 12.5378\n",
      "Epoch [2/3], Step [38200/41412], Loss: 2.1339, Perplexity: 8.44755\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.4388, Perplexity: 11.4593\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.1573, Perplexity: 8.64810\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.4194, Perplexity: 11.2390\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.4683, Perplexity: 11.8018\n",
      "Epoch [2/3], Step [38700/41412], Loss: 3.1679, Perplexity: 23.7571\n",
      "Epoch [2/3], Step [38800/41412], Loss: 1.8878, Perplexity: 6.60509\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.2070, Perplexity: 9.08887\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.5693, Perplexity: 13.0566\n",
      "Epoch [2/3], Step [39100/41412], Loss: 1.8063, Perplexity: 6.08799\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.3371, Perplexity: 10.3509\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.3427, Perplexity: 10.40932\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.2808, Perplexity: 9.78486\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.8839, Perplexity: 17.8837\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.4066, Perplexity: 11.0967\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.4984, Perplexity: 12.1625\n",
      "Epoch [2/3], Step [39800/41412], Loss: 1.9900, Perplexity: 7.31590\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.0506, Perplexity: 7.77298\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.2736, Perplexity: 9.71456\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.0348, Perplexity: 7.65048\n",
      "Epoch [2/3], Step [40200/41412], Loss: 2.2712, Perplexity: 9.69133\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.4582, Perplexity: 11.6840\n",
      "Epoch [2/3], Step [40400/41412], Loss: 3.0838, Perplexity: 21.8411\n",
      "Epoch [2/3], Step [40500/41412], Loss: 2.0792, Perplexity: 7.99784\n",
      "Epoch [2/3], Step [40600/41412], Loss: 3.0696, Perplexity: 21.5333\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.3818, Perplexity: 10.8246\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.6140, Perplexity: 13.6531\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.5966, Perplexity: 13.4178\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.1272, Perplexity: 8.39145\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.3769, Perplexity: 10.7715\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.8643, Perplexity: 17.5359\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.0549, Perplexity: 7.80600\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.5281, Perplexity: 12.5292\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.2947, Perplexity: 9.9210012\n",
      "Epoch [3/3], Step [200/41412], Loss: 1.7433, Perplexity: 5.71615\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.4159, Perplexity: 11.1997\n",
      "Epoch [3/3], Step [400/41412], Loss: 2.2770, Perplexity: 9.74744\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.4810, Perplexity: 11.9538\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.2879, Perplexity: 9.85409\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.1626, Perplexity: 8.69386\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.5454, Perplexity: 12.7478\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.3094, Perplexity: 10.0682\n",
      "Epoch [3/3], Step [1000/41412], Loss: 2.3020, Perplexity: 9.9939\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.3375, Perplexity: 10.3556\n",
      "Epoch [3/3], Step [1200/41412], Loss: 2.2098, Perplexity: 9.11373\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.3956, Perplexity: 10.9750\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.0566, Perplexity: 7.81979\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.1636, Perplexity: 8.70246\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.4072, Perplexity: 11.1025\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.2138, Perplexity: 9.15081\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.4983, Perplexity: 12.1616\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.1652, Perplexity: 8.71671\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.1175, Perplexity: 8.31079\n",
      "Epoch [3/3], Step [2100/41412], Loss: 1.6908, Perplexity: 5.42383\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.4247, Perplexity: 11.2984\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.5705, Perplexity: 13.0723\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.5173, Perplexity: 12.3952\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.4517, Perplexity: 11.6082\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.2632, Perplexity: 9.61374\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.4866, Perplexity: 12.0201\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.4020, Perplexity: 11.0452\n",
      "Epoch [3/3], Step [2900/41412], Loss: 2.2576, Perplexity: 9.55993\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.2677, Perplexity: 9.65739\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.4092, Perplexity: 11.1250\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.4610, Perplexity: 11.7169\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.0317, Perplexity: 7.62734\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.2166, Perplexity: 9.17652\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.4470, Perplexity: 11.5537\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.2080, Perplexity: 9.09798\n",
      "Epoch [3/3], Step [3700/41412], Loss: 2.7170, Perplexity: 15.1343\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.5952, Perplexity: 13.3993\n",
      "Epoch [3/3], Step [3900/41412], Loss: 2.1262, Perplexity: 8.38323\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.2072, Perplexity: 9.09011\n",
      "Epoch [3/3], Step [4100/41412], Loss: 1.9324, Perplexity: 6.90611\n",
      "Epoch [3/3], Step [4200/41412], Loss: 2.6449, Perplexity: 14.0823\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.2893, Perplexity: 9.86778\n",
      "Epoch [3/3], Step [4400/41412], Loss: 1.6346, Perplexity: 5.12742\n",
      "Epoch [3/3], Step [4500/41412], Loss: 3.0444, Perplexity: 20.9975\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.6694, Perplexity: 14.4315\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.4482, Perplexity: 11.5679\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.8596, Perplexity: 17.4552\n",
      "Epoch [3/3], Step [4900/41412], Loss: 2.3458, Perplexity: 10.4417\n",
      "Epoch [3/3], Step [5000/41412], Loss: 2.2802, Perplexity: 9.77894\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.7141, Perplexity: 15.0903\n",
      "Epoch [3/3], Step [5200/41412], Loss: 3.0639, Perplexity: 21.4113\n",
      "Epoch [3/3], Step [5300/41412], Loss: 2.4817, Perplexity: 11.9616\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.1393, Perplexity: 8.49385\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.3309, Perplexity: 10.2873\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.4996, Perplexity: 12.1780\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.8925, Perplexity: 18.0385\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.5012, Perplexity: 12.1973\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.8109, Perplexity: 16.6243\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.9272, Perplexity: 18.6760\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.2623, Perplexity: 9.60522\n",
      "Epoch [3/3], Step [6200/41412], Loss: 1.9257, Perplexity: 6.86026\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.3645, Perplexity: 10.6388\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.7220, Perplexity: 15.2104\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.3639, Perplexity: 10.6328\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.1064, Perplexity: 8.21830\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.0405, Perplexity: 7.69424\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.2382, Perplexity: 9.37693\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.5782, Perplexity: 13.1740\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.1261, Perplexity: 8.38215\n",
      "Epoch [3/3], Step [7100/41412], Loss: 2.2011, Perplexity: 9.03521\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.2698, Perplexity: 9.67796\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.0078, Perplexity: 7.44693\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.1991, Perplexity: 9.01693\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.9276, Perplexity: 18.6832\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.1496, Perplexity: 8.58110\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.5246, Perplexity: 12.4861\n",
      "Epoch [3/3], Step [7800/41412], Loss: 2.3620, Perplexity: 10.6119\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.2708, Perplexity: 9.68765\n",
      "Epoch [3/3], Step [8000/41412], Loss: 2.0892, Perplexity: 8.07872\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.3786, Perplexity: 10.7898\n",
      "Epoch [3/3], Step [8200/41412], Loss: 2.5045, Perplexity: 12.2378\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.0503, Perplexity: 7.77032\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.8724, Perplexity: 17.6791\n",
      "Epoch [3/3], Step [8500/41412], Loss: 2.1815, Perplexity: 8.85952\n",
      "Epoch [3/3], Step [8600/41412], Loss: 1.9908, Perplexity: 7.32135\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.1672, Perplexity: 8.73429\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.6251, Perplexity: 13.8065\n",
      "Epoch [3/3], Step [8900/41412], Loss: 2.7208, Perplexity: 15.1924\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.2607, Perplexity: 9.59029\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.0186, Perplexity: 7.52770\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.9683, Perplexity: 19.4587\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.1640, Perplexity: 8.70595\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.5457, Perplexity: 12.7523\n",
      "Epoch [3/3], Step [9500/41412], Loss: 2.9525, Perplexity: 19.1544\n",
      "Epoch [3/3], Step [9600/41412], Loss: 2.2572, Perplexity: 9.55632\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.3199, Perplexity: 10.1749\n",
      "Epoch [3/3], Step [9800/41412], Loss: 1.7788, Perplexity: 5.92286\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.3871, Perplexity: 10.8817\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.3182, Perplexity: 10.1571\n",
      "Epoch [3/3], Step [10100/41412], Loss: 2.0392, Perplexity: 7.68480\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.9056, Perplexity: 18.2761\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.1664, Perplexity: 8.72671\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.5771, Perplexity: 13.1589\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.2930, Perplexity: 9.90446\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.0745, Perplexity: 7.96062\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.4395, Perplexity: 11.4667\n",
      "Epoch [3/3], Step [10800/41412], Loss: 2.4911, Perplexity: 12.0750\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.7658, Perplexity: 15.8918\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.1154, Perplexity: 8.29319\n",
      "Epoch [3/3], Step [11100/41412], Loss: 1.8521, Perplexity: 6.37330\n",
      "Epoch [3/3], Step [11200/41412], Loss: 2.2496, Perplexity: 9.48439\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.3578, Perplexity: 10.5677\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.4129, Perplexity: 11.1666\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.4747, Perplexity: 11.8783\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.7249, Perplexity: 15.2550\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.9428, Perplexity: 18.9681\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.8123, Perplexity: 16.6487\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.4824, Perplexity: 11.9696\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.5799, Perplexity: 13.1962\n",
      "Epoch [3/3], Step [12100/41412], Loss: 1.8988, Perplexity: 6.67815\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.2199, Perplexity: 9.20667\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.9789, Perplexity: 7.23466\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.0031, Perplexity: 7.41236\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.3243, Perplexity: 10.2194\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.0374, Perplexity: 7.67106\n",
      "Epoch [3/3], Step [12700/41412], Loss: 1.9436, Perplexity: 6.98366\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.4878, Perplexity: 12.0343\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.1270, Perplexity: 8.38936\n",
      "Epoch [3/3], Step [13000/41412], Loss: 2.2781, Perplexity: 9.75835\n",
      "Epoch [3/3], Step [13100/41412], Loss: 2.0772, Perplexity: 7.98220\n",
      "Epoch [3/3], Step [13200/41412], Loss: 2.2244, Perplexity: 9.24785\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.1546, Perplexity: 8.62466\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.1188, Perplexity: 8.32146\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.2862, Perplexity: 9.83738\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.2305, Perplexity: 9.30429\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.2462, Perplexity: 9.45150\n",
      "Epoch [3/3], Step [13800/41412], Loss: 3.0063, Perplexity: 20.2128\n",
      "Epoch [3/3], Step [13900/41412], Loss: 2.2913, Perplexity: 9.887775\n",
      "Epoch [3/3], Step [14000/41412], Loss: 2.4909, Perplexity: 12.0719\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.4680, Perplexity: 11.7990\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.8835, Perplexity: 17.8770\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.8225, Perplexity: 16.8187\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.3590, Perplexity: 10.5804\n",
      "Epoch [3/3], Step [14500/41412], Loss: 3.5088, Perplexity: 33.4092\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.3607, Perplexity: 10.5989\n",
      "Epoch [3/3], Step [14700/41412], Loss: 1.9160, Perplexity: 6.79400\n",
      "Epoch [3/3], Step [14800/41412], Loss: 2.1656, Perplexity: 8.71982\n",
      "Epoch [3/3], Step [14900/41412], Loss: 1.9560, Perplexity: 7.07128\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.5334, Perplexity: 12.5959\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.9973, Perplexity: 20.0318\n",
      "Epoch [3/3], Step [15200/41412], Loss: 1.6210, Perplexity: 5.05834\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.3273, Perplexity: 10.2498\n",
      "Epoch [3/3], Step [15400/41412], Loss: 2.3728, Perplexity: 10.7275\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.2387, Perplexity: 9.38128\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.6027, Perplexity: 13.5007\n",
      "Epoch [3/3], Step [15700/41412], Loss: 1.9985, Perplexity: 7.37806\n",
      "Epoch [3/3], Step [15800/41412], Loss: 1.9369, Perplexity: 6.93753\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.0592, Perplexity: 7.83967\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.9687, Perplexity: 7.16106\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.3890, Perplexity: 10.9028\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.5564, Perplexity: 12.8896\n",
      "Epoch [3/3], Step [16300/41412], Loss: 2.2556, Perplexity: 9.54146\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.6531, Perplexity: 14.1978\n",
      "Epoch [3/3], Step [16500/41412], Loss: 2.4549, Perplexity: 11.6455\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.1090, Perplexity: 8.24002\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.1425, Perplexity: 8.52072\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.7182, Perplexity: 15.1533\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.9669, Perplexity: 19.4319\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.0737, Perplexity: 7.95415\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.2088, Perplexity: 9.10478\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.2238, Perplexity: 9.24254\n",
      "Epoch [3/3], Step [17300/41412], Loss: 2.2324, Perplexity: 9.32209\n",
      "Epoch [3/3], Step [17400/41412], Loss: 2.3147, Perplexity: 10.1222\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.0067, Perplexity: 7.43902\n",
      "Epoch [3/3], Step [17600/41412], Loss: 1.6943, Perplexity: 5.44287\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.4820, Perplexity: 11.9658\n",
      "Epoch [3/3], Step [17800/41412], Loss: 1.6624, Perplexity: 5.27175\n",
      "Epoch [3/3], Step [17900/41412], Loss: 2.3320, Perplexity: 10.2981\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.5795, Perplexity: 13.1911\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.1754, Perplexity: 8.80593\n",
      "Epoch [3/3], Step [18200/41412], Loss: 2.2532, Perplexity: 9.51793\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.2428, Perplexity: 9.41982\n",
      "Epoch [3/3], Step [18400/41412], Loss: 2.3375, Perplexity: 10.3553\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.3328, Perplexity: 10.3064\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.1924, Perplexity: 8.95700\n",
      "Epoch [3/3], Step [18700/41412], Loss: 2.3151, Perplexity: 10.1255\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.0976, Perplexity: 8.14659\n",
      "Epoch [3/3], Step [18900/41412], Loss: 1.7518, Perplexity: 5.76481\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.3219, Perplexity: 10.1952\n",
      "Epoch [3/3], Step [19100/41412], Loss: 1.6029, Perplexity: 4.96755\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.2210, Perplexity: 9.216100\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.2330, Perplexity: 9.32762\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.2316, Perplexity: 9.31513\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.1973, Perplexity: 9.00042\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.2027, Perplexity: 9.04950\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.0111, Perplexity: 7.47175\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.4495, Perplexity: 11.5831\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.2055, Perplexity: 9.07518\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.3715, Perplexity: 10.7138\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.2816, Perplexity: 9.792596\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.4393, Perplexity: 11.4650\n",
      "Epoch [3/3], Step [20300/41412], Loss: 2.4182, Perplexity: 11.2262\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.6298, Perplexity: 13.8714\n",
      "Epoch [3/3], Step [20500/41412], Loss: 2.3061, Perplexity: 10.0354\n",
      "Epoch [3/3], Step [20600/41412], Loss: 2.2756, Perplexity: 9.73368\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.1340, Perplexity: 8.448554\n",
      "Epoch [3/3], Step [20800/41412], Loss: 2.4286, Perplexity: 11.3433\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.4738, Perplexity: 11.8680\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.1068, Perplexity: 8.22154\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.5532, Perplexity: 12.8482\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.7026, Perplexity: 14.9188\n",
      "Epoch [3/3], Step [21300/41412], Loss: 2.8153, Perplexity: 16.6986\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.5461, Perplexity: 12.7567\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.0587, Perplexity: 7.83587\n",
      "Epoch [3/3], Step [21600/41412], Loss: 2.1696, Perplexity: 8.75521\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.4465, Perplexity: 11.5484\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.6106, Perplexity: 13.6068\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.3701, Perplexity: 10.6990\n",
      "Epoch [3/3], Step [22000/41412], Loss: 1.9928, Perplexity: 7.33648\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.2616, Perplexity: 9.59833\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.8758, Perplexity: 17.7397\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.3627, Perplexity: 10.6201\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.3958, Perplexity: 10.9770\n",
      "Epoch [3/3], Step [22500/41412], Loss: 1.9837, Perplexity: 7.26942\n",
      "Epoch [3/3], Step [22600/41412], Loss: 3.2272, Perplexity: 25.2089\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.4962, Perplexity: 12.1361\n",
      "Epoch [3/3], Step [22800/41412], Loss: 2.2008, Perplexity: 9.03196\n",
      "Epoch [3/3], Step [22900/41412], Loss: 1.9816, Perplexity: 7.25469\n",
      "Epoch [3/3], Step [23000/41412], Loss: 3.1832, Perplexity: 24.1232\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.1102, Perplexity: 8.24985\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.4264, Perplexity: 11.3178\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.3097, Perplexity: 10.0718\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.3550, Perplexity: 10.5379\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.3548, Perplexity: 10.5362\n",
      "Epoch [3/3], Step [23600/41412], Loss: 3.4613, Perplexity: 31.8592\n",
      "Epoch [3/3], Step [23700/41412], Loss: 1.7620, Perplexity: 5.82400\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.1001, Perplexity: 8.16731\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.2489, Perplexity: 9.47733\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.0862, Perplexity: 8.05431\n",
      "Epoch [3/3], Step [24100/41412], Loss: 2.4247, Perplexity: 11.2988\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.0293, Perplexity: 7.60882\n",
      "Epoch [3/3], Step [24300/41412], Loss: 1.6686, Perplexity: 5.30475\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.2764, Perplexity: 9.74162\n",
      "Epoch [3/3], Step [24500/41412], Loss: 1.8273, Perplexity: 6.21704\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.5263, Perplexity: 12.5070\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.2463, Perplexity: 9.45280\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.9581, Perplexity: 19.2617\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.5157, Perplexity: 12.3752\n",
      "Epoch [3/3], Step [25000/41412], Loss: 1.9727, Perplexity: 7.19047\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.4905, Perplexity: 12.0678\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.2987, Perplexity: 9.96179\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.5188, Perplexity: 12.4131\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.0787, Perplexity: 7.99385\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.2981, Perplexity: 9.95506\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.6964, Perplexity: 14.8261\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.1611, Perplexity: 8.680978\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.5068, Perplexity: 12.2655\n",
      "Epoch [3/3], Step [25900/41412], Loss: 2.5697, Perplexity: 13.0620\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.0201, Perplexity: 7.53920\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.3788, Perplexity: 10.7919\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.4909, Perplexity: 12.0727\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.3819, Perplexity: 10.8260\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.8388, Perplexity: 17.0955\n",
      "Epoch [3/3], Step [26500/41412], Loss: 1.7895, Perplexity: 5.98643\n",
      "Epoch [3/3], Step [26600/41412], Loss: 2.4746, Perplexity: 11.8773\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.0503, Perplexity: 7.77000\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.3064, Perplexity: 10.0378\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.6074, Perplexity: 13.5643\n",
      "Epoch [3/3], Step [27000/41412], Loss: 3.0735, Perplexity: 21.6185\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.3089, Perplexity: 10.0635\n",
      "Epoch [3/3], Step [27200/41412], Loss: 2.5989, Perplexity: 13.4495\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.5005, Perplexity: 12.1880\n",
      "Epoch [3/3], Step [27400/41412], Loss: 2.1820, Perplexity: 8.86384\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.1845, Perplexity: 8.88647\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.3522, Perplexity: 10.5082\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.6315, Perplexity: 13.89390\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.6027, Perplexity: 13.5000\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.6432, Perplexity: 14.0583\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.3254, Perplexity: 10.2307\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.7727, Perplexity: 16.0015\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.3118, Perplexity: 10.0929\n",
      "Epoch [3/3], Step [28300/41412], Loss: 2.3269, Perplexity: 10.2465\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.2572, Perplexity: 9.55674\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.7933, Perplexity: 16.3356\n",
      "Epoch [3/3], Step [28600/41412], Loss: 2.3627, Perplexity: 10.6199\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.3922, Perplexity: 10.9374\n",
      "Epoch [3/3], Step [28800/41412], Loss: 2.3876, Perplexity: 10.8875\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.1741, Perplexity: 8.79418\n",
      "Epoch [3/3], Step [29000/41412], Loss: 2.5113, Perplexity: 12.3213\n",
      "Epoch [3/3], Step [29100/41412], Loss: 1.7280, Perplexity: 5.62947\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.0436, Perplexity: 7.71841\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.2349, Perplexity: 9.34541\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.4249, Perplexity: 11.3006\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.0840, Perplexity: 8.03696\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.2737, Perplexity: 9.71530\n",
      "Epoch [3/3], Step [29700/41412], Loss: 2.7759, Perplexity: 16.05363\n",
      "Epoch [3/3], Step [29800/41412], Loss: 2.6197, Perplexity: 13.7310\n",
      "Epoch [3/3], Step [29900/41412], Loss: 2.3114, Perplexity: 10.0890\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.5349, Perplexity: 12.6154\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.0946, Perplexity: 8.12196\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.5215, Perplexity: 12.4473\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.4010, Perplexity: 11.0342\n",
      "Epoch [3/3], Step [30400/41412], Loss: 2.5500, Perplexity: 12.8071\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.7459, Perplexity: 15.5783\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.7257, Perplexity: 15.2664\n",
      "Epoch [3/3], Step [30700/41412], Loss: 1.9583, Perplexity: 7.08748\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.9023, Perplexity: 18.2167\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.0316, Perplexity: 7.62650\n",
      "Epoch [3/3], Step [31000/41412], Loss: 2.1416, Perplexity: 8.51352\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.5125, Perplexity: 12.3363\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.5535, Perplexity: 12.8519\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.6614, Perplexity: 14.3166\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.8244, Perplexity: 16.8504\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.2256, Perplexity: 9.25915\n",
      "Epoch [3/3], Step [31600/41412], Loss: 3.1131, Perplexity: 22.4914\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.7215, Perplexity: 15.2036\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.2155, Perplexity: 9.16596\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.3406, Perplexity: 10.3875\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.3392, Perplexity: 10.3729\n",
      "Epoch [3/3], Step [32100/41412], Loss: 2.6314, Perplexity: 13.89304\n",
      "Epoch [3/3], Step [32200/41412], Loss: 2.9148, Perplexity: 18.4445\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.2963, Perplexity: 9.93743\n",
      "Epoch [3/3], Step [32400/41412], Loss: 1.9493, Perplexity: 7.02385\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.0533, Perplexity: 7.79375\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.2304, Perplexity: 9.30356\n",
      "Epoch [3/3], Step [32700/41412], Loss: 2.4306, Perplexity: 11.3662\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.6164, Perplexity: 13.6864\n",
      "Epoch [3/3], Step [32900/41412], Loss: 2.3585, Perplexity: 10.5753\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.3525, Perplexity: 10.5114\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.0690, Perplexity: 7.91692\n",
      "Epoch [3/3], Step [33200/41412], Loss: 3.0570, Perplexity: 21.2646\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.1847, Perplexity: 8.88800\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.2037, Perplexity: 9.05879\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.2117, Perplexity: 9.13113\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.4550, Perplexity: 11.6461\n",
      "Epoch [3/3], Step [33700/41412], Loss: 2.4245, Perplexity: 11.2965\n",
      "Epoch [3/3], Step [33800/41412], Loss: 2.6525, Perplexity: 14.1893\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.8701, Perplexity: 17.6383\n",
      "Epoch [3/3], Step [34000/41412], Loss: 2.1749, Perplexity: 8.80160\n",
      "Epoch [3/3], Step [34100/41412], Loss: 3.0326, Perplexity: 20.7511\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.2835, Perplexity: 9.81065\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.1893, Perplexity: 8.92876\n",
      "Epoch [3/3], Step [34400/41412], Loss: 2.1492, Perplexity: 8.57813\n",
      "Epoch [3/3], Step [34500/41412], Loss: 2.3984, Perplexity: 11.0056\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.1569, Perplexity: 8.64392\n",
      "Epoch [3/3], Step [34700/41412], Loss: 2.4534, Perplexity: 11.6282\n",
      "Epoch [3/3], Step [34800/41412], Loss: 2.1111, Perplexity: 8.25708\n",
      "Epoch [3/3], Step [34900/41412], Loss: 3.3358, Perplexity: 28.0996\n",
      "Epoch [3/3], Step [35000/41412], Loss: 2.3852, Perplexity: 10.8611\n",
      "Epoch [3/3], Step [35100/41412], Loss: 3.0164, Perplexity: 20.41681\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.5855, Perplexity: 13.2698\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.0739, Perplexity: 7.95613\n",
      "Epoch [3/3], Step [35400/41412], Loss: 2.1156, Perplexity: 8.29493\n",
      "Epoch [3/3], Step [35500/41412], Loss: 2.0181, Perplexity: 7.52386\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.7456, Perplexity: 15.5732\n",
      "Epoch [3/3], Step [35700/41412], Loss: 2.6126, Perplexity: 13.6351\n",
      "Epoch [3/3], Step [35800/41412], Loss: 2.9172, Perplexity: 18.4898\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.2580, Perplexity: 9.56420\n",
      "Epoch [3/3], Step [36000/41412], Loss: 2.6492, Perplexity: 14.1428\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.9773, Perplexity: 7.22336\n",
      "Epoch [3/3], Step [36200/41412], Loss: 1.8698, Perplexity: 6.48692\n",
      "Epoch [3/3], Step [36300/41412], Loss: 2.1806, Perplexity: 8.85189\n",
      "Epoch [3/3], Step [36400/41412], Loss: 2.3691, Perplexity: 10.6876\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.4024, Perplexity: 11.0496\n",
      "Epoch [3/3], Step [36600/41412], Loss: 1.8893, Perplexity: 6.61454\n",
      "Epoch [3/3], Step [36700/41412], Loss: 2.5064, Perplexity: 12.2604\n",
      "Epoch [3/3], Step [36800/41412], Loss: 2.1548, Perplexity: 8.62628\n",
      "Epoch [3/3], Step [36900/41412], Loss: 1.8888, Perplexity: 6.61137\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.3712, Perplexity: 10.7098\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.3027, Perplexity: 10.0014\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.5708, Perplexity: 13.0766\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.4040, Perplexity: 11.0674\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.4588, Perplexity: 11.6913\n",
      "Epoch [3/3], Step [37500/41412], Loss: 4.1039, Perplexity: 60.5740\n",
      "Epoch [3/3], Step [37600/41412], Loss: 2.5546, Perplexity: 12.8666\n",
      "Epoch [3/3], Step [37700/41412], Loss: 2.4661, Perplexity: 11.7762\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.4746, Perplexity: 11.8773\n",
      "Epoch [3/3], Step [37900/41412], Loss: 2.1671, Perplexity: 8.73290\n",
      "Epoch [3/3], Step [38000/41412], Loss: 2.0724, Perplexity: 7.94399\n",
      "Epoch [3/3], Step [38100/41412], Loss: 2.2508, Perplexity: 9.49510\n",
      "Epoch [3/3], Step [38200/41412], Loss: 1.9166, Perplexity: 6.79758\n",
      "Epoch [3/3], Step [38300/41412], Loss: 3.0241, Perplexity: 20.5746\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.4931, Perplexity: 12.0987\n",
      "Epoch [3/3], Step [38500/41412], Loss: 2.2442, Perplexity: 9.43333\n",
      "Epoch [3/3], Step [38600/41412], Loss: 2.1914, Perplexity: 8.94736\n",
      "Epoch [3/3], Step [38700/41412], Loss: 3.0058, Perplexity: 20.2020\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.6015, Perplexity: 13.4842\n",
      "Epoch [3/3], Step [38900/41412], Loss: 2.0796, Perplexity: 8.00155\n",
      "Epoch [3/3], Step [39000/41412], Loss: 2.9089, Perplexity: 18.3373\n",
      "Epoch [3/3], Step [39100/41412], Loss: 1.9239, Perplexity: 6.84752\n",
      "Epoch [3/3], Step [39200/41412], Loss: 1.8677, Perplexity: 6.47314\n",
      "Epoch [3/3], Step [39300/41412], Loss: 2.2307, Perplexity: 9.30622\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.3152, Perplexity: 10.1274\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.3140, Perplexity: 10.1145\n",
      "Epoch [3/3], Step [39600/41412], Loss: 2.6623, Perplexity: 14.3289\n",
      "Epoch [3/3], Step [39700/41412], Loss: 2.5424, Perplexity: 12.7101\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.3862, Perplexity: 10.8717\n",
      "Epoch [3/3], Step [39900/41412], Loss: 2.6191, Perplexity: 13.7232\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.1647, Perplexity: 8.71206\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.5874, Perplexity: 13.2953\n",
      "Epoch [3/3], Step [40200/41412], Loss: 2.2426, Perplexity: 9.41811\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.2800, Perplexity: 9.77683\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.2397, Perplexity: 9.39010\n",
      "Epoch [3/3], Step [40500/41412], Loss: 2.0251, Perplexity: 7.57682\n",
      "Epoch [3/3], Step [40600/41412], Loss: 2.3443, Perplexity: 10.4260\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.0119, Perplexity: 7.47771\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.6451, Perplexity: 14.0845\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.3046, Perplexity: 10.0202\n",
      "Epoch [3/3], Step [41000/41412], Loss: 2.1199, Perplexity: 8.33061\n",
      "Epoch [3/3], Step [41100/41412], Loss: 2.5160, Perplexity: 12.3793\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.2347, Perplexity: 9.34364\n",
      "Epoch [3/3], Step [41300/41412], Loss: 2.4822, Perplexity: 11.9681\n",
      "Epoch [3/3], Step [41400/41412], Loss: 2.2672, Perplexity: 9.65251\n",
      "Epoch [3/3], Step [41412/41412], Loss: 2.1488, Perplexity: 8.57485"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "#         if time.time() - old_time > 60:\n",
    "#             old_time = time.time()\n",
    "#             requests.request(\"POST\", \n",
    "#                              \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "#                              headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
