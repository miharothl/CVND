#!/usr/bin/env python3
import argparse

import torch
from matplotlib import transforms
from torch.utils.data import DataLoader

from cv import drl_logger
# from cv.experiment.analyser import Analyzer
# from cv.experiment.configuration import Configuration
# from cv.experiment.experiment import Experiment
# from cv.experiment.explorer import Explorer
from cv.data_load import FacialKeypointsDataset, Rescale, RandomCrop, Normalize, ToTensor
from cv.logging import init_logging, set_logging_level, transform_verbose_count_to_logging_level
from val.bleu.bleu import Bleu


def parse_arguments(params):
    ap = argparse.ArgumentParser(description="Reinforcement Learning Lab",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("-v", "--verbose", dest="verbose_count",
                    action="count", default=0,
                    help="Increases log verbosity for each occurrence.")

    args = ap.parse_args()

    return args


def train():
    ################################################################################################################
    ################################################################################################################

    import torch
    import torch.nn as nn
    from torchvision import transforms
    import sys
    sys.path.append('/Users/rothlmi/scm/r/hub/cocoapi/PythonAPI')
    from pycocotools.coco import COCO
    from data_loader import get_loader
    from model import EncoderCNN, DecoderRNN
    import math
    from torch.optim import Adam

    ## TODO #1: Select appropriate values for the Python variables below.
    batch_size = 10  # batch size
    vocab_threshold = 10  # minimum word count threshold
    vocab_from_file = True  # if True, load existing vocab file
    embed_size = 300  # dimensionality of image and word embeddings
    hidden_size = 128  # number of features in hidden state of the RNN decoder

    num_epochs = 3  # number of training epochs
    save_every = 1  # determines frequency of saving model weights
    print_every = 100  # determines window for printing average loss
    log_file = 'training_log.txt'  # name of file with saved training loss and perplexity

    # (Optional) TODO #2: Amend the image transform below.
    transform_train = transforms.Compose([
        transforms.Resize(256),  # smaller edge of image resized to 256
        transforms.RandomCrop(224),  # get 224x224 crop from random location
        transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
        transforms.ToTensor(),  # convert the PIL Image to a tensor
        transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model
                             (0.229, 0.224, 0.225))])

    # Build data loader.
    data_loader = get_loader(transform=transform_train,
                             mode='train',
                             batch_size=batch_size,
                             vocab_threshold=vocab_threshold,
                             vocab_from_file=vocab_from_file,
                             cocoapi_loc='/Users/rothlmi/scm/r/hub/')

    # The size of the vocabulary.
    vocab_size = len(data_loader.dataset.vocab)

    # Initialize the encoder and decoder.
    encoder = EncoderCNN(embed_size)
    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)

    # Move models to GPU if CUDA is available.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    encoder.to(device)
    decoder.to(device)

    # Define the loss function.
    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()

    # TODO #3: Specify the learnable parameters of the model.
    params = list(decoder.parameters()) + list(encoder.embed.parameters())

    # TODO #4: Define the optimizer.
    optimizer = Adam(params, lr=0.001)

    # Set the total number of training steps per epoch.
    total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)

    ################################################################################################################
    ################################################################################################################

    import torch.utils.data as data
    import numpy as np
    import os
    import requests
    import time

    # Open the training log file.
    f = open(log_file, 'w')

    old_time = time.time()
    # response = requests.request("GET",
    #                             "http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token",
    #                             headers={"Metadata-Flavor":"Google"})

    for epoch in range(1, num_epochs + 1):

        for i_step in range(1, total_step + 1):

            #         if time.time() - old_time > 60:
            #             old_time = time.time()
            #             requests.request("POST",
            #                              "https://nebula.udacity.com/api/v1/remote/keep-alive",
            #                              headers={'Authorization': "STAR " + response.text})

            # Randomly sample a caption length, and sample indices with that length.
            indices = data_loader.dataset.get_train_indices()
            # Create and assign a batch sampler to retrieve a batch with the sampled indices.
            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)
            data_loader.batch_sampler.sampler = new_sampler

            # Obtain the batch.
            images, captions = next(iter(data_loader))

            # Move batch of images and captions to GPU if CUDA is available.
            images = images.to(device)
            captions = captions.to(device)

            # Zero the gradients.
            decoder.zero_grad()
            encoder.zero_grad()

            # Pass the inputs through the CNN-RNN model.
            features = encoder(images)
            outputs = decoder(features, captions)

            # Calculate the batch loss.
            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))

            # Backward pass.
            loss.backward()

            # Update the parameters in the optimizer.
            optimizer.step()

            # Get training statistics.
            stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (
                epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))

            # Print training statistics (on same line).
            print('\r' + stats, end="")
            sys.stdout.flush()

            # Print training statistics to file.
            f.write(stats + '\n')
            f.flush()

            # Print training statistics (on different line).
            if i_step % print_every == 0:
                print('\r' + stats)

        # Save the weights.
        if epoch % save_every == 0:
            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))
            torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))

    # Close the training log file.
    f.close()


def eval():
    # (Optional) TODO: Validate your model.

    import sys
    sys.path.append('/Users/rothlmi/scm/r/hub/cocoapi/PythonAPI')
    from pycocotools.coco import COCO
    from data_loader import get_loader
    from torchvision import transforms

    # TODO #1: Define a transform to pre-process the testing images.
    transform_val = transforms.Compose([
        transforms.Resize(256),  # smaller edge of image resized to 256
        transforms.RandomCrop(224),  # get 224x224 crop from random location
        transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
        transforms.ToTensor(),  # convert the PIL Image to a tensor
        transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model
                             (0.229, 0.224, 0.225))])

    #-#-#-# Do NOT modify the code below this line. #-#-#-#

    # Create the data loader.
    from data_loader_val import get_loader
    data_loader = get_loader(transform=transform_val,
                             mode='val',
                             cocoapi_loc='/Users/rothlmi/scm/r/hub/')

    import torch
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ################################################################################################################
    ################################################################################################################
    import os
    import torch
    from model import EncoderCNN
    from model import DecoderRNN

    # TODO #2: Specify the saved models to load.
    encoder_file = 'encoder-3.pkl'
    decoder_file = 'decoder-3.pkl'

    # TODO #3: Select appropriate values for the Python variables below.
    embed_size  = 300          # dimensionality of image and word embeddings
    hidden_size = 128          # number of features in hidden state of the RNN decoder

    # The size of the vocabulary.
    vocab_size = len(data_loader.dataset.vocab)

    # Initialize the encoder and decoder, and set each to inference mode.
    encoder = EncoderCNN(embed_size)
    encoder.eval()
    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)
    decoder.eval()

    # TODO #2: Specify the saved models to load.
    encoder_file = 'encoder-1.pkl'
    decoder_file = 'decoder-1.pkl'
    # Load the trained weights.
    encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file), map_location=torch.device('cpu')))
    decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file), map_location=torch.device('cpu')))

    # Move models to GPU if CUDA is available.
    encoder.to(device)
    decoder.to(device)

    score, scores = bleu_eval(encoder, decoder, data_loader, device, 50)
    print(score)

    # TODO #2: Specify the saved models to load.
    encoder_file = 'encoder-2.pkl'
    decoder_file = 'decoder-2.pkl'
    # Load the trained weights.
    encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file), map_location=torch.device('cpu')))
    decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file), map_location=torch.device('cpu')))

    # Move models to GPU if CUDA is available.
    encoder.to(device)
    decoder.to(device)

    score, scores = bleu_eval(encoder, decoder, data_loader, device, 50)
    print(score)

    # TODO #2: Specify the saved models to load.
    encoder_file = 'encoder-3.pkl'
    decoder_file = 'decoder-3.pkl'
    # Load the trained weights.
    encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file), map_location=torch.device('cpu')))
    decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file), map_location=torch.device('cpu')))

    # Move models to GPU if CUDA is available.
    encoder.to(device)
    decoder.to(device)

    score, scores = bleu_eval(encoder, decoder, data_loader, device, 50)
    print(score)


def bleu_eval(encoder, decoder, data_loader, device, number_of_random_val_images):

    # TODO #4: Complete the function.
    def clean_sentence(output):

        words = []
        sentence = ""

        for i in output:
            if (i == 0):
                continue

            if (i == 1):
                break
            words.append(data_loader.dataset.vocab.idx2word[i])

        sentence = ' '.join(words)

        sentence = sentence.capitalize()

        return sentence

    ################################################################################################################
    ################################################################################################################

    i = 0

    from collections import defaultdict

    res = defaultdict(list)
    gts = defaultdict(list)

    for img_id, img, caption in data_loader:

        if i >= number_of_random_val_images:
            break

        img = img.to(device)
        with torch.no_grad():
            features = encoder(img).unsqueeze(1)
            output = decoder.sample(features)

        caption = caption.squeeze(0).numpy().tolist()

        pred_caption = clean_sentence(output)
        caption = clean_sentence(caption)
        print(img_id.item())
        print(caption)
        print(pred_caption)

        res[img_id.item()] = [pred_caption]
        gts[img_id.item()] = [caption]

        i = i + 1

    ################################################################################################################
    ################################################################################################################

    bleu_scorer = Bleu()

    score, scores = bleu_scorer.compute_score(gts, res)
    # score, scores = bleu_scorer.compute_score(gts, gts)

    return score, scores

def interference():

    ################################################################################################################
    ################################################################################################################
    import sys
    # sys.path.append('/opt/cocoapi/PythonAPI')
    sys.path.append('/Users/rothlmi/scm/r/hub/cocoapi/PythonAPI')
    from pycocotools.coco import COCO
    from data_loader import get_loader
    from torchvision import transforms

    # TODO #1: Define a transform to pre-process the testing images.
    transform_test = transforms.Compose([
        transforms.Resize(256),  # smaller edge of image resized to 256
        transforms.RandomCrop(224),  # get 224x224 crop from random location
        transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
        transforms.ToTensor(),  # convert the PIL Image to a tensor
        transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model
                             (0.229, 0.224, 0.225))])

    #-#-#-# Do NOT modify the code below this line. #-#-#-#

    # Create the data loader.
    data_loader = get_loader(transform=transform_test,
                             mode='test',
                             cocoapi_loc='/Users/rothlmi/scm/r/hub/')

    ################################################################################################################
    ################################################################################################################

    import numpy as np
    import matplotlib.pyplot as plt
    #% matplotlib inline

    # Obtain sample image before and after pre-processing.
    orig_image, image = next(iter(data_loader))

    # Visualize sample image, before pre-processing.
    plt.imshow(np.squeeze(orig_image))
    plt.title('example image')
    plt.show()

    ################################################################################################################
    ################################################################################################################

    import torch

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ################################################################################################################
    ################################################################################################################

    # Watch for any changes in model.py, and re-load it automatically.
    # % load_ext autoreload
    # % autoreload 2

    import os
    import torch
    from model import EncoderCNN, DecoderRNN

    # TODO #2: Specify the saved models to load.
    encoder_file = 'encoder-3.pkl'
    decoder_file = 'decoder-3.pkl'

    # TODO #3: Select appropriate values for the Python variables below.
    embed_size  = 300          # dimensionality of image and word embeddings
    hidden_size = 128          # number of features in hidden state of the RNN decoder

    # The size of the vocabulary.
    vocab_size = len(data_loader.dataset.vocab)

    # Initialize the encoder and decoder, and set each to inference mode.
    encoder = EncoderCNN(embed_size)
    encoder.eval()
    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)
    decoder.eval()

    # Load the trained weights.
    encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file), map_location=torch.device('cpu')))
    decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file), map_location=torch.device('cpu')))

    # Move models to GPU if CUDA is available.
    encoder.to(device)
    decoder.to(device)

    ################################################################################################################
    ################################################################################################################

    # Move image Pytorch Tensor to GPU if CUDA is available.
    image = image.to(device)

    # Obtain the embedded image features.
    features = encoder(image).unsqueeze(1)

    # Pass the embedded image features through the model to get a predicted caption.
    output = decoder.sample(features)
    print('example output:', output)

    assert (type(output) == list), "Output needs to be a Python list"
    assert all([type(x) == int for x in output]), "Output should be a list of integers."
    assert all([x in data_loader.dataset.vocab.idx2word for x in
                output]), "Each entry in the output needs to correspond to an integer that indicates a token in the vocabulary."


    pass


def main():
    init_logging()

    # params = Configuration().get_app_config()
    params = {}

    try:
        args = parse_arguments(params)

        set_logging_level(transform_verbose_count_to_logging_level(args.verbose_count))

        drl_logger.info(
            "Arguments.",
            extra={"params": {
                "arguments": params,
            }})

        if False:
            train()

        if True:
            eval()

        if False:
            interference()


    except Exception as e:
        drl_logger.exception(
            "Something went wrong :-(",
            extra={"params": {
                "exception": e,
            }})

    finally:
        pass


if __name__ == '__main__':
    main()

